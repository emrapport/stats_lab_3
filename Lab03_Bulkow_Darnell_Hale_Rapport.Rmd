---
title: "Lab 3: Reducing Crime"
subtitle: "w203 Summer 2018"
author: "Madeleine Bulkow, Kim Darnell, Alla Hale, Emily Rapport"
date: \today
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction
As advisees to candidates running for statewide office in North Carolina, we believe that the crime rate across the state should be of central concern to any candidate.  Local governments across the state desire to control the crime rate, and rigorous data analysis is needed to understand the role of crime in different parts of the state. This report examines the available crime data and attempts to answer the following research question: what variables are associated with crime rates across counties in North Carolina?  Based on this analysis, we generate several policy suggestions applicable to local government in North Carolina for the late 1980s.

## 2. Data Definitions and Data Cleaning
The data in this report was collected by researchers Cornewell and Trumball. They collected data related to crime, demographics, and the economy for 97 counties in North Carolina. While the authors collected data over a number of years, we will focus on the data from the year 1987. 
 
The dataset includes the following variables, which we present with definitions and assumptions:

county: integer code representing which county the row represents. We received the data with these identifier codes in place of county names, so we cannot identify the individual counties in the dataset.

year: 1987 for all data points. 

crmrte: ratio of crimes permitted to population, taken from the FBI's Uniform Crime Reports.

prbarr: ratio of arrests to offenses, taken from the FBI's Uniform Crime Reports.

prbconv: ratio of convictions to arrests. Arrest data is taken from the FBI's Uniform Crime Reports, while conviction data is taken from the North Carolina Department of Correction.

prbpris: ratio of prison sentences to convictions, taken from the North Carolina Department of Correction.

avgsen: average prison sentence in days, which we believe is taken from the North Carolina Department of Correction.

polpc: police per capita, computed using the FBI's police agency employee counts.

density: people per square mile

taxpc: tax revenue per capita. 

west: indicator code specifying whether county is in Western North Carolina (1 if yes, 0 if no).

central: indicator code specifying whether county is in Central North Carolina (1 if yes, 0 if no).

urban: indicator code specifying whether county is urban, defined by whether the county is in a Standard Metropolitan Statistical Area as defined by the US Census. 

pctmin80: percentage of population that belongs to minority racial group, as taken by the 1980 US Census.

mix: ratio of face-to-face offenses to other offenses.

pctymle: percent young male, defined as proportion of population that is male between the ages of 15 and 24, as taken by US Census data. 

The remaining variables represent weekly wages in particular industries, as provided by the North Carolina Employment Security Commission:
- wcon: construction
- wtuc: transit, utilities, and communication
- wtrd: wholesale, retail trade
- wfir: finance, insurance, real estate
- wser: service industry
- wmfg: manufacturing
- wfed: federal employees
- wsta: state employees
- wloc: local government employees
 
We start by evaluating the available data, cleaning it by removing anomolous values, and perhaps transforming the data.  
```{r}
# Import the data
df = read.csv("crime_v2.csv")
#summary(df)
```

Clean up the apostrophe.

It appears that probconv is in percent, while the other two probability estimates (prbarr and prbpris) are fractions. To be able to compare coefficients more easily, let's get all percentage values in percent (0-100).

Remove the points where probabilities exceed 100 %.

```{r}
# Clean the data

## NOTE FROM ALLA: This is just what I did to clean the data.  I am sure this can be done in a more efficient way.  Please just let me know what the final df is called so that I can update it in my models.
df_calc <- df
df_calc$prbconv <- as.numeric(as.numeric(df$prbconv))
df_calc$prbarr <- df$prbarr * 100
df_calc$prbpris <- df$prbpris * 100
df_calc$pctymle <- df$pctymle * 100
#summary(df_calc)
df_clean <-df_calc[with(df_calc, prbarr <= 100 & wser <= 2000),]
summary(df_clean)
```

##3. Building Models
Our central goal for this analysis is to determine what variables are most associated with crime in North Carolina. For this reason, we will  use the crmrte variable as the outcome variable in most our models. Before we begin modeling, we need to examine our outcome variable.

```{r}
summary(df_clean$crmrte)
```

The value of crmrte ranges from approximately .011 to .099, with a mean of approximately .034. Six observations have NA values in place of their crmrte. Since our analysis primarily seeks to discover variables associated with crmrte, we will remove these rows with NA's, as they will not serve our analysis. 

```{r}
df_clean <- df_clean[complete.cases(df_clean$crmrte), ]
```

We are left with 89 observations. A histogram of the observations shows us the shape of their distribution:

```{r}
hist(df_clean$crmrte, main="Crime Rate for Individual Counties", xlab= "Crime Rate", ylab= "Number of Counties")
```

We see in this histogram that the data is right skewed; the majority of county's crime rates are below .04, while the long right tail demonstrates that a smaller number of counties have substantially higher crime rates. 

## 4. The Models
### 4.1 Model 1
```{r}
# Build Model 1
# model 1: things that totally make sense and have good r^2. density, taxpc, pctymle.
(model_1 = lm(crmrte ~ density + taxpc + pctymle, data = df_clean))
summary(model_1)$r.square
plot(model_1, which = 5)
#(model_1 = lm(crmrte ~ prbarr + log(prbconv) + log(polpc) + density + taxpc + pctmin80 + pctymle , data = df_clean))
```

### 4.2 Model 2
```{r}
# Build Model 2
# model 2: other things that are explanatory but maybe questionable: west, polpc, arrest/conviction, ppctmin80. 
(model_2 = lm(crmrte ~ density + taxpc + pctymle + west + log(polpc) + prbarr + prbconv, data = df_clean))
summary(model_2)$r.square
plot(model_2, which = 5)
```

### 4.3 Model 3
```{r}
# Build Model 3
#model 3: not necessarily explanatory, but not problematic: central, avgsen, prison.
(model_3 = lm(crmrte ~ density + taxpc + pctymle + west + log(polpc) + prbarr + prbconv + central + avgsen + prbpris, data = df_clean))
summary(model_3)$r.square
plot(model_3, which = 5)
```
### 4.4 Model 4
```{r}
# Build Model 4
# model 4: kitchen sink. urban, wage.
(model_4 = lm(crmrte ~ density + taxpc + pctymle + west + log(polpc) + prbarr + prbconv + central + avgsen + prbpris + wcon + wtuc + wtrd + wfir + wser + wmfg + wfed + wsta + wloc, data = df_clean))
summary(model_4)$r.square
plot(model_4, which = 5)
```

### 4.5 Model 5
```{r}
# Build Model 5
# model 5: the model 1 version of a model for this dependent variable - crmrate*mix
```

### 4.2 Model Summary
This is where we put our model summary table.
```{r, results='asis'}
library(stargazer)
stargazer(model_1, model_2, model_3, model_4, type = "latex", 
          report = "vc", # Don't report errors, since we haven't covered them
          title = "Linear Models Predicting Crime Rate",
          keep.stat = c("rsq", "n"),
          omit.table.layout = "n") # Omit more output related to errors
```

## 5. Omitted Variables

## 6. Conclusion
- might be worth making a point about county as unit : might make sense since county likely determines different police/judicial jurisdictions, but certain elements in our model might not be consistent across whole county (i.e. density, tax rate in cities, etc.)
