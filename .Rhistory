resid2b <- resid(model2b)
# regress forest on the residuals for the explanatory variables
model2c <- lm(resid2a ~ forest)
length(resid2a)
length(resid2b)
length(forest)
# regress forest on the residuals for the explanatory variables
model2c <- lm(resid2a ~ forest)
# regress explanatory variables 1 and 2 on explanatory variable 3
model2a <- lm(gdp ~ milexp)
model2b <- lm(gdp ~ agrawexp)
# extract the residuals
resid2a <- resid(model2a)
resid2b <- resid(model2b)
# generate a quantile normal plot of the residuals to examine the distribution
qqnorm(resid2a, main = "Residuals for MILEXP Regressed on GDP")
qqline(resid2a)
qqnorm(resid2b, main = "Residuals for AGRAWEXP Regressed on GDP")
qqline(resid2b)
summary(model2)
cor(forest, milexp, use = "complete.obs" )
cor(forest, milexp, use = "complete.obs" )
cor(forest, agrawexp, use = "complete.obs" )
cor(forest, milexp, use = "complete.obs" )
cor(forest, agrawexp, use = "complete.obs" )
cor(forest, gdp, use = "complete.obs" )
knitr::opts_chunk$set(echo = TRUE)
# regress explanatory variables 1 and 2 on explanatory variable 3
model2a <- lm(gdp ~ milexp)
# regress explanatory variables 1 and 2 on explanatory variable 3
model2a <- lm(gdp ~ milexp)
knitr::opts_chunk$set(echo = TRUE)
load("G:/MIDS/Statistics for Data Science/Unit 11/Week11.Rdata")
ls(Data)
ls(Definitions)
hist(Data$AG.LND.FRST.ZS,
main = "Percent of Forested Land for WB Countries",
xlab = "Percent of Forested Land",
ylab = "# WB Countries")
summary(Data$AG.LND.FRST.ZS)
length(Data$AG.LND.FRST.ZS)
hist(Data$MS.MIL.XPND.GD.ZS,
main = "Percent GDP for Military Expenditures",
xlab = "Percent of GDP for Military Expenditures",
ylab = "Number of WB Countries")
summary(Data$MS.MIL.XPND.GD.ZS)
length(Data$MS.MIL.XPND.GD.ZS)
hist(Data$MS.MIL.XPND.ZS,
main = "Percent General Expenditures for Military",
xlab = "Percent General Expenditures for Military",
ylab = "Number of WB Countries")
summary(Data$MS.MIL.XPND.ZS)
hist(Data$NY.GDP.MKTP.CD,
main = "Current GDP in US Dollars",
xlab = "Current GDP in US Dollars",
ylab = "Number of WB Countries")
summary(Data$NY.GDP.MKTP.CD)
hist(Data$NY.GDP.PCAP.CD,
main = "Current GDP per Capita in US Dollars",
xlab = "Current GDP per Capita in US Dollars",
ylab = "Number of WB Countries")
summary(Data$NY.GDP.PCAP.CD)
hist(Data$NY.GDP.PETR.RT.ZS,
main = "Percent GDP for Oil Rents",
xlab = "Percent GDP for Oil Rents",
ylab = "Number of WB Countries")
summary(Data$NY.GDP.PETR.RT.ZS)
hist(Data$MS.MIL.XPRT.KD,
main = "SIPRI Arms Exports",
xlab = "SIPRI Arms Exports",
ylab = "Number of WB Countries")
summary(Data$MS.MIL.XPRT.KD)
hist(Data$TX.VAL.AGRI.ZS.UN,
main = "Agricultural Raw Materials Exports",
xlab = "Agricultural Raw Materials Exports",
ylab = "Number of WB Countries")
summary(Data$TX.VAL.AGRI.ZS.UN)
hist(Data$MS.MIL.MPRT.KD,
main = "SIPRI Arms Imports",
xlab = "SIPRI Arms Imports",
ylab = "Number of WB Countries")
summary(Data$MS.MIL.MPRT.KD)
hist(Data$NE.IMP.GNFS.CD,
main = "Imports of Goods/Services US$",
xlab = "Imports of Goods/Services US$",
ylab = "Number of WB Countries")
summary(Data$NE.IMP.GNFS.CD)
hist(Data$NE.EXP.GNFS.CD,
main = "Exports of Goods/Services US$",
xlab = "Exports of Goods/Services US$",
ylab = "Number of WB Countries")
summary(Data$NE.EXP.GNFS.CD)
apply(!is.na(Data[,-(1:2)] ) , MARGIN= 2, mean )
cor(Data$NE.IMP.GNFS.CD, Data$NE.EXP.GNFS.CD, use = "complete.obs" )
# Rename the variable
forest <- Data$AG.LND.FRST.ZS
# Rename the explanatory variables of interest
milexp <- Data$MS.MIL.XPND.GD.ZS
agrawexp <- Data$TX.VAL.AGRI.ZS.UN
# Check to see if the independent variables are correlated (weak correlation = good!)
cor(milexp, agrawexp, use = "complete.obs" )
# Examine the correlations between the explanatory variables and the variable of interest
cor(forest, milexp, use = "complete.obs" )
cor(forest, agrawexp, use = "complete.obs" )
# build a model and evaluate r-squared and the Cook's Distances for the data points
model1 = lm(forest ~ milexp + agrawexp, data = Data)
summary(model1)$r.square
plot(model1, which = 5)
# Evaluate the assumption about exogeneity.  I want this value to be 0.
round(sum(model1$residuals * model1$fitted.values), 10)
summary(model1)
# Rename the explanatory variables of interest
gdp <- Data$NY.GDP.MKTP.CD
# Examine the correlations between the explanatory variables
cor(milexp, gdp, use = "complete.obs" )
cor(agrawexp, gdp, use = "complete.obs" )
# build a model and evaluate r-squared and the Cook's Distances for the data points
model2 <- lm(forest ~ milexp + agrawexp + gdp, data = Data)
summary(model2)$r.square
plot(model2, which = 5)
summary(model2)
# regress explanatory variables 1 and 2 on explanatory variable 3
model2a <- lm(gdp ~ milexp)
model2b <- lm(gdp ~ agrawexp)
# extract the residuals
resid2a <- resid(model2a)
resid2b <- resid(model2b)
# generate a quantile normal plot of the residuals to examine the distribution
qqnorm(resid2a, main = "Residuals for MILEXP Regressed on GDP")
qqline(resid2a)
qqnorm(resid2b, main = "Residuals for AGRAWEXP Regressed on GDP")
qqline(resid2b)
# regress forest on the residuals for the explanatory variables
#model2c <- lm(resid2a ~ forest)
#model2d <- lm(resid2b ~ forest)
length(resid2a)
length(resid2b)
length(forest)
# generate a quantile normal plot of the residuals to examine the distribution
#qqnorm(resid2a, main = "Residuals for MILEXP Regressed on GDP")
#qqline(resid2a)
#qqnorm(resid2b, main = "Residuals for AGRAWEXP Regressed on GDP")
#qqline(resid2b)
cor(forest, milexp, use = "complete.obs" )
cor(forest, agrawexp, use = "complete.obs" )
cor(forest, gdp, use = "complete.obs" )
knitr::opts_chunk$set(echo = TRUE)
load("G:/MIDS/Statistics for Data Science/Unit 11/Week11.Rdata")
ls(Data)
ls(Definitions)
hist(Data$AG.LND.FRST.ZS,
main = "Percent of Forested Land for WB Countries",
xlab = "Percent of Forested Land",
ylab = "# WB Countries")
summary(Data$AG.LND.FRST.ZS)
length(Data$AG.LND.FRST.ZS)
hist(Data$MS.MIL.XPND.GD.ZS,
main = "Percent GDP for Military Expenditures",
xlab = "Percent of GDP for Military Expenditures",
ylab = "Number of WB Countries")
summary(Data$MS.MIL.XPND.GD.ZS)
length(Data$MS.MIL.XPND.GD.ZS)
hist(Data$MS.MIL.XPND.ZS,
main = "Percent General Expenditures for Military",
xlab = "Percent General Expenditures for Military",
ylab = "Number of WB Countries")
summary(Data$MS.MIL.XPND.ZS)
hist(Data$NY.GDP.MKTP.CD,
main = "Current GDP in US Dollars",
xlab = "Current GDP in US Dollars",
ylab = "Number of WB Countries")
summary(Data$NY.GDP.MKTP.CD)
hist(Data$NY.GDP.PCAP.CD,
main = "Current GDP per Capita in US Dollars",
xlab = "Current GDP per Capita in US Dollars",
ylab = "Number of WB Countries")
summary(Data$NY.GDP.PCAP.CD)
hist(Data$NY.GDP.PETR.RT.ZS,
main = "Percent GDP for Oil Rents",
xlab = "Percent GDP for Oil Rents",
ylab = "Number of WB Countries")
summary(Data$NY.GDP.PETR.RT.ZS)
hist(Data$MS.MIL.XPRT.KD,
main = "SIPRI Arms Exports",
xlab = "SIPRI Arms Exports",
ylab = "Number of WB Countries")
summary(Data$MS.MIL.XPRT.KD)
hist(Data$TX.VAL.AGRI.ZS.UN,
main = "Agricultural Raw Materials Exports",
xlab = "Agricultural Raw Materials Exports",
ylab = "Number of WB Countries")
summary(Data$TX.VAL.AGRI.ZS.UN)
hist(Data$MS.MIL.MPRT.KD,
main = "SIPRI Arms Imports",
xlab = "SIPRI Arms Imports",
ylab = "Number of WB Countries")
summary(Data$MS.MIL.MPRT.KD)
hist(Data$NE.IMP.GNFS.CD,
main = "Imports of Goods/Services US$",
xlab = "Imports of Goods/Services US$",
ylab = "Number of WB Countries")
summary(Data$NE.IMP.GNFS.CD)
hist(Data$NE.EXP.GNFS.CD,
main = "Exports of Goods/Services US$",
xlab = "Exports of Goods/Services US$",
ylab = "Number of WB Countries")
summary(Data$NE.EXP.GNFS.CD)
apply(!is.na(Data[,-(1:2)] ) , MARGIN= 2, mean )
cor(Data$NE.IMP.GNFS.CD, Data$NE.EXP.GNFS.CD, use = "complete.obs" )
# Rename the variable
forest <- Data$AG.LND.FRST.ZS
# Rename the explanatory variables of interest
milexp <- Data$MS.MIL.XPND.GD.ZS
agrawexp <- Data$TX.VAL.AGRI.ZS.UN
# Check to see if the independent variables are correlated (weak correlation = good!)
cor(milexp, agrawexp, use = "complete.obs" )
# Examine the correlations between the explanatory variables and the variable of interest
cor(forest, milexp, use = "complete.obs" )
cor(forest, agrawexp, use = "complete.obs" )
# build a model and evaluate r-squared and the Cook's Distances for the data points
model1 = lm(forest ~ milexp + agrawexp, data = Data)
summary(model1)$r.square
plot(model1, which = 5)
# Evaluate the assumption about exogeneity.  I want this value to be 0.
round(sum(model1$residuals * model1$fitted.values), 10)
summary(model1)
# Rename the explanatory variables of interest
gdp <- Data$NY.GDP.MKTP.CD
# Examine the correlations between the explanatory variables
cor(milexp, gdp, use = "complete.obs" )
cor(agrawexp, gdp, use = "complete.obs" )
# build a model and evaluate r-squared and the Cook's Distances for the data points
model2 <- lm(forest ~ milexp + agrawexp + gdp, data = Data)
summary(model2)$r.square
plot(model2, which = 5)
summary(model2)
# regress explanatory variables 1 and 2 on explanatory variable 3
model2a <- lm(gdp ~ milexp)
model2b <- lm(gdp ~ agrawexp)
# extract the residuals
resid2a <- resid(model2a)
resid2b <- resid(model2b)
# generate a quantile normal plot of the residuals to examine the distribution
qqnorm(resid2a, main = "Residuals for MILEXP Regressed on GDP")
qqline(resid2a)
qqnorm(resid2b, main = "Residuals for AGRAWEXP Regressed on GDP")
qqline(resid2b)
# regress forest on the residuals for the explanatory variables
#model2c <- lm(resid2a ~ forest)
#model2d <- lm(resid2b ~ forest)
length(resid2a)
length(resid2b)
length(forest)
# generate a quantile normal plot of the residuals to examine the distribution
#qqnorm(resid2a, main = "Residuals for MILEXP Regressed on GDP")
#qqline(resid2a)
#qqnorm(resid2b, main = "Residuals for AGRAWEXP Regressed on GDP")
#qqline(resid2b)
cor(forest, milexp, use = "complete.obs" )
cor(forest, agrawexp, use = "complete.obs" )
cor(forest, gdp, use = "complete.obs" )
# regress explanatory variables 1 and 2 on explanatory variable 3
model2a <- lm(gdp ~ milexp)
model2b <- lm(gdp ~ agrawexp)
# extract the residuals
resid2a <- resid(model2a)
resid2b <- resid(model2b)
# generate a quantile normal plot of the residuals to examine the distribution
qqnorm(resid2a, main = "Residuals for MILEXP Regressed on GDP")
qqline(resid2a)
qqnorm(resid2b, main = "Residuals for AGRAWEXP Regressed on GDP")
qqline(resid2b)
knitr::opts_chunk$set(echo = TRUE)
library(stargazer)
# Import car package
library(car)
# Import the data
df = read.csv("crime_v2.csv")
# Clean the data
## Reassign the dataframe to a working variable
df_calc <- df
# Convert the prbarr, prbpris, and pctymle variables from decimals to percentages
df_calc$prbarr <- df$prbarr * 100
df_calc$prbpris <- df$prbpris * 100
df_calc$pctymle <- df$pctymle * 100
# Convert the mix variable from decimals to percentage
df_calc$mix <- df$mix * 100
# Convert the polpc variable from decimals to number of police per 1000 people
df_calc$polpc <- df$polpc * 1000
# Convert the prbconv variable from integer to numeric
df_calc$prbconv <- as.numeric(levels(df$prbconv)[df$prbconv])
#remove row 89, which is a duplicate of row 88 (Madison County, FIPS 193)
df_clean <- df_calc[-c(89), ]
#remove rows with no data (i.e., all NA values)
df_clean <- df_clean[-c(91:97), ]
summary(df_clean$crmrte)
length(df_clean$crmrte)
hist(df_clean$crmrte,
main="County Crime Rates in 1987 North Carolina",
xlab= "Crimes Committed per Person",
ylab= "Number of Counties")
# Calculate the face-to-face crime rate
df_clean$ftfcrmrte <- df_clean$crmrte * (1-1/(df_clean$mix+1))
# Examine the distribution
summary(df_clean$ftfcrmrte)
hist(df_clean$ftfcrmrte,
main="Face to Face County Crime Rates in 1987 North Carolina",
xlab= "Crimes Committed per Person",
ylab= "Number of Counties")
df_clean$crmrte_ratio <- 1-1/(df_clean$mix+1)
hist(df_clean$crmrte_ratio,
main= "County Crime Rate Ratio in 1987 in North Carolina",
xlab= "Face to Face Crimes: Total Crimes",
ylab= "Number of Counties")
summary(df_clean$density)
hist(df_clean$density,
main="Population Densities across NC Counties",
xlab= "Population Density (1/100)",
ylab= "Number of Counties",
breaks = 15)
summary(df_clean$taxpc)
hist(df_clean$taxpc,
main="Cumulative Tax per Capita Across Counties",
xlab= "Local and State Tax per Capita (1/100)",
ylab= "Number of Counties",
breaks = 30)
summary(df_clean$pctymle)
hist(df_clean$pctymle,
main= " Percent Males Ages 15-24 across Counties",
xlab= "Percent Males Ages 15-24",
ylab= "Number of Counties",
breaks = 30)
vars <- c("crmrte", "density", "taxpc","pctymle")
suppressWarnings(scatterplotMatrix(df_clean[,vars], diagonal = list(method= "histogram")))
# Build Model 1
model_1 = lm(crmrte ~ density + taxpc + pctymle, data = df_clean)
summary(model_1)$r.square
plot(model_1, which = 5)
round(sum(model_1$residuals * model_1$fitted.values), 15)
plot(model_1, which = 1)
vars <- c("polpc", "prbarr", "prbconv","pctmin80")
suppressWarnings(scatterplotMatrix(df_clean[,vars], diagonal = list(method = "histogram")))
# Build Model 2
model_2 = lm(crmrte ~ density + taxpc + pctymle
+ west + polpc + prbarr + prbconv + pctmin80,
data = df_clean)
summary(model_2)$r.square
plot(model_2, which = 5)
round(sum(model_2$residuals * model_2$fitted.values), 15)
plot(df_clean$urban , df_clean$density,
main= "Urban Density",
ylab= "Density (100 people per square mile)",
xlab= "Urban")
abline(lm(density ~ urban, data = df_clean))
# Build Model 3
model_3 = lm(crmrte ~ density + taxpc + pctymle
+ west + polpc + prbarr + prbconv + pctmin80
+ central + avgsen + prbpris,
data = df_clean)
summary(model_3)$r.square
plot(model_3, which = 5)
round(sum(model_3$residuals * model_3$fitted.values), 15)
# Build Model 4
# model 4: kitchen sink. urban, wage.
model_4 = lm(crmrte ~ density + taxpc + pctymle
+ west + polpc + prbarr + prbconv + pctmin80
+ central + avgsen + prbpris
+ urban + wcon + wtuc + wtrd + wfir + wser
+ wmfg + wfed + wsta + wloc + mix,
data = df_clean)
summary(model_4)$r.square
plot(model_4, which = 5)
plot(crmrte ~ ftfcrmrte, data = df_clean)
# Build Model 5
model_5 <- lm(ftfcrmrte ~ density + taxpc + pctymle, data=df_clean)
plot(model_5, which = 5)
summary(model_5)$r.squared
stargazer(model_1, model_2, model_3, model_4, model_5, type = "latex",
report = "vc", # Don't report errors, since we haven't covered them
title = "4.6.1 Linear Models Predicting Crime Rate",
keep.stat = c("rsq", "n"),
omit.table.layout = "n") # Omit more output related to errors
# Calculate the face-to-face crime rate
df_clean$ftfcrmrte <- df_clean$crmrte * (1-1/(df_clean$mix+1))
# Examine the distribution
summary(df_clean$ftfcrmrte)
hist(df_clean$ftfcrmrte,
main="Face-to-Face Crime Rates",
xlab= "Crimes Committed per Person",
ylab= "Number of Counties")
df_clean$crmrte_ratio <- 1-1/(df_clean$mix+1)
hist(df_clean$crmrte_ratio,
main= "County Crime Rates in 1987 North Carolina",
xlab= "Face-to-Face Crimes/Total Crimes",
ylab= "Number of Counties")
knitr::opts_chunk$set(echo = TRUE)
library(stargazer)
# Import car package
library(car)
# Import the data
df = read.csv("crime_v2.csv")
# Clean the data
## Reassign the dataframe to a working variable
df_calc <- df
# Convert the prbarr, prbpris, and pctymle variables from decimals to percentages
df_calc$prbarr <- df$prbarr * 100
df_calc$prbpris <- df$prbpris * 100
df_calc$pctymle <- df$pctymle * 100
# Convert the mix variable from decimals to percentage
df_calc$mix <- df$mix * 100
# Convert the polpc variable from decimals to number of police per 1000 people
df_calc$polpc <- df$polpc * 1000
# Convert the prbconv variable from integer to numeric
df_calc$prbconv <- as.numeric(levels(df$prbconv)[df$prbconv])
#remove row 89, which is a duplicate of row 88 (Madison County, FIPS 193)
df_clean <- df_calc[-c(89), ]
#remove rows with no data (i.e., all NA values)
df_clean <- df_clean[-c(91:97), ]
summary(df_clean$crmrte)
length(df_clean$crmrte)
hist(df_clean$crmrte,
main="County Crime Rates in 1987 North Carolina",
xlab= "Crimes Committed per Person",
ylab= "Number of Counties")
# Calculate the face-to-face crime rate
df_clean$ftfcrmrte <- df_clean$crmrte * (1-1/(df_clean$mix+1))
# Examine the distribution
summary(df_clean$ftfcrmrte)
hist(df_clean$ftfcrmrte,
main="Face-to-Face Crime Rates",
xlab= "Crimes Committed per Person",
ylab= "Number of Counties")
df_clean$crmrte_ratio <- 1-1/(df_clean$mix+1)
hist(df_clean$crmrte_ratio,
main= "County Crime Rates in 1987 North Carolina",
xlab= "Face-to-Face Crimes/Total Crimes",
ylab= "Number of Counties")
##Note from Kim: I think we really need to move the general descriptions of all of our CLM assumptions here, then indicate clearly as we go through the each model when we are doing things to demonstrate that an assumption has or has not been met.  We want the client to anticipate that we are going to be thorough, systematic, and transparent.  We may want to cite Wooldrige (2016) and introduce the client to some of the basic symbols and terms we will use.  This will make it easier for us to describe things in a more formal, professional way. We can't assume, for example, that the client will know what *u* or "statistical error" is.
summary(df_clean$density)
hist(df_clean$density,
main="Population Densities across NC Counties",
xlab= "Population Density (1/100)",
ylab= "Number of Counties",
breaks = 15)
summary(df_clean$taxpc)
hist(df_clean$taxpc,
main="Tax per Capita Across NC Counties",
xlab= "Local and State Tax per Capita (1/100)",
ylab= "Number of Counties",
breaks = 30)
summary(df_clean$pctymle)
hist(df_clean$pctymle,
main= " Percent Males Ages 15-24 across Counties",
xlab= "Percent Males Ages 15-24",
ylab= "Number of Counties",
breaks = 30)
vars <- c("crmrte", "density", "taxpc","pctymle")
suppressWarnings(scatterplotMatrix(df_clean[,vars], diagonal = list(method= "histogram")))
# Build Model 1
model_1 = lm(crmrte ~ density + taxpc + pctymle, data = df_clean)
summary(model_1)$r.square
plot(model_1, which = 5)
round(sum(model_1$residuals * model_1$fitted.values), 15)
plot(model_1, which = 1)
plot(model_1, which= 2)
vars <- c("polpc", "prbarr", "prbconv","pctmin80")
suppressWarnings(scatterplotMatrix(df_clean[,vars],
diagonal = list(method = "histogram")))
# Build Model 2
model_2 = lm(crmrte ~ density + taxpc + pctymle
+ west + polpc + prbarr + prbconv + pctmin80,
data = df_clean)
summary(model_2)$r.square
plot(model_2, which = 5)
round(sum(model_2$residuals * model_2$fitted.values), 15)
plot(model_2, which= 2)
plot(df_clean$urban , df_clean$density,
main= "Urban Density",
ylab= "Density (100 people per square mile)",
xlab= "Urban")
abline(lm(density ~ urban, data = df_clean))
# Build Model 3
model_3 = lm(crmrte ~ density + taxpc + pctymle
+ west + polpc + prbarr + prbconv + pctmin80
+ central + avgsen + prbpris,
data = df_clean)
summary(model_3)$r.square
plot(model_3, which= 5)
round(sum(model_3$residuals * model_3$fitted.values), 15)
# Build Model 4
# model 4: kitchen sink. urban, wage.
model_4 = lm(crmrte ~ density + taxpc + pctymle
+ west + polpc + prbarr + prbconv + pctmin80
+ central + avgsen + prbpris
+ urban + wcon + wtuc + wtrd + wfir + wser
+ wmfg + wfed + wsta + wloc + mix,
data = df_clean)
summary(model_4)$r.square
plot(model_4, which = 5)
plot(crmrte ~ ftfcrmrte, data = df_clean)
# Build Model 5
model_5 <- lm(ftfcrmrte ~ density + taxpc + pctymle, data=df_clean)
plot(model_5, which = 5)
summary(model_5)$r.squared
stargazer(model_1, model_2, model_3, model_4, model_5, type = "latex",
report = "vc", # Don't report errors, since we haven't covered them
title = "4.6.1 Linear Models Predicting Crime Rate",
keep.stat = c("rsq", "n"),
omit.table.layout = "n") # Omit more output related to errors
crime_v2 <- read.csv("C:/Users/drdcr/w203/stats_lab_3/crime_v2.csv")
View(crime_v2)
View(crime_v2)
